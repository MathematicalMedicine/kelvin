<h1>Polynomial Compilation</h1>
<h2>How Does K<font size="+2">ELVIN</font> use Polynomials?</h2>
In order to understand the value of polynomial compilation, one must first understand what polynomials
are and how K<font size="-1">ELVIN</font> uses them.
<p>
When K<font size="-1">ELVIN</font> performs an analysis, the majority of the work is calculating the likelihood of every pedigree's structure
and characteristics for each combination of values in the trait model space, which is typically a range of 
6 gene frequencies, 275 penetrances, and 20 alpha values, or 33,000 combinations of values. This is repeated for
each trait position in the analysis.
<p>
In (original) non-polynomial mode, the pedigree likelihood is explicitly calculated
via a series of tests, loops and arithmetic operations in the compiled code that comprises K<font size="-1">ELVIN</font>.
Since these tests, loops and operations are all hard-coded in the program, they cannot be 
optimized or simplified in response to the nature of each pedigree, so the same full series of steps is followed
for every combination of values.
<p>
In polynomial mode, we go thru this same hard-coded path in K<font size="-1">ELVIN</font> <i>one time only</i> building a 
symbolic representation of the
calculation using place-holder variables from the trait model space instead of actual numeric values. In effect, we
build a polynomial that represents the structure and characteristics of the pedigree in terms of the variables from
the trait model space. This polynomial representation can then be extensively simplified and optimized 
using mathematical and computational techniques so that when we assign trait model space values to the place-holder
variables and evaluate it, it can perform the calculation 
orders of magnitude faster than the original hard-coded path in K<font size="-1">ELVIN</font>.
<p>
This process of generating polynomials has its own drawbacks. They can take a lot of time and memory to generate
and optimize, and even when
optimization is complete, their evaluation is symbolic and therefore slower than the equivalent binary 
operations. Ideally, we'd like to have both the optimized simplicity of the pedigree/position-specific 
polynomial <i>and</i> the performance and
reusability of compiled code. The solution is to generate and compile code representing each polynomial.
<h2>What Polynomials are Built?</h2>
During a polynomial-mode  analysis, a separate set of likelihood polynomials is built for each pedigree and combination of
relative trait/marker positions. For example, a multipoint analysis with
two pedigrees and three markers where evaluated trait positions occur between every pair of markers as well as before
and after the markers would generate twelve polynomials:
<ul>
<li>trait likelihood for pedigree 1
<li>trait likelihood for pedigree 2
<li>marker set likelihood for M1, M2, M3 and pedigree 1
<li>marker set likelihood for M1, M2, M3 and pedigree 2
<li>combined likelihood for pedigree 1 and trait/marker pattern T-M1-M2-M3
<li>combined likelihood for pedigree 2 and trait/marker pattern T-M1-M2-M3
<li>combined likelihood for pedigree 1 and trait/marker pattern M1-T-M2-M3
<li>combined likelihood for pedigree 2 and trait/marker pattern M1-T-M2-M3
<li>combined likelihood for pedigree 1 and trait/marker pattern M1-M2-T-M3
<li>combined likelihood for pedigree 2 and trait/marker pattern M1-M2-T-M3
<li>combined likelihood for pedigree 1 and trait/marker pattern M1-M2-M3-T
<li>combined likelihood for pedigree 2 and trait/marker pattern M1-M2-M3-T
</ul>
The trait and marker likelihood polynomials are extremely simple compared to the combined
likelihood polynomials. Marker likelihood polynomials usually simplify to a constant, while
combined likelihood polynomials usually have tens of thousands, and sometimes even billions of terms.
<p>
<h2>What is Polynomial Compilation?</h2>
Polynomial compilation is a method of post-processing a discrete, namable, in-memory polynomial generated by K<font size="-1">ELVIN</font>
so as to facilitate rapid evaluation and reuse. By discrete and namable, we mean that all information
required for the construction of the polynomial is available before construction begins, and that there are
a relatively small number of attributes that can be used to distinguish this particular polynomial from other 
similar ones and therefore comprise a unique name.
Fortunately, K<font size="-1">ELVIN</font> has been written in such a way that all major polynomials generated are both
discrete and namable.
<p>
When polynomial compilation is used, likelihood polynomials are built in-memory as usual in an initial 
K<font size="-1">ELVIN</font> run, and then translated into C-language code 
and written to one or
more source files which are then compiled into dynamic libraries. These dynamic libraries are
reusable, distributable, compiled representations of the original polynomials that can be
loaded on-demand and evaluated up to ten times faster than their original in-memory counterparts.
<p>
Once
a dynamic library is built for a given polynomial, the relatively slow and memory-intensive process of generating
that polynomial need not be performed again unless one of the following changes:
<ul>
<li>Count or positions of markers
<li>Structure of pedigrees
<li>Phenotypic or genotypic characteristics of individuals
<li>Type of analysis such as LD vs LE (two-point and multipoint analyses create differently-named
polynomials and can therefore co-exist).
</ul>
While compiled polynomial names incorporate enough information to uniquely identify them <i>within a
given analysis run</i>, they do not contain an exhaustive description of the analysis, so it is 
important to be careful to remove any existing compiled polynomials (<tt>*.so</tt> files)
when one of the aforementioned characteristics
changes, otherwise K<font size="-1">ELVIN</font> might crash, or worse -- the analysis results will be incorrect.
You can, however, freely change any of the following analysis characteristics and still take advantage
of any pre-existing compiled polynomials:
<ul>
<li>Pedigrees included/excluded (new ones can be compiled or just built and used without compilation)
<li>Trait position values for multipoint analysis (may require additional polynomials)
<li>Gene frequency values
<li>Penetrance values
<li>Alpha values
<li>D-prime values for LD analysis
</ul>

<h2>When to Use Polynomial Compilation</h2>
Generally, you should use polynomial compilation only when it is technically feasible in your 
computational environment and
the cost and complexity of compiling and linking dynamic libraries is exceeded by the benefits of rapid
and repeatable evaluation.
<h3>Indications</h3>
<ul>
<li><b>When the ratio of evaluations to trait/marker position changes is high.</b> This is an indication to use
polynomial evaluation in general, and compilation in particular, as the costs of generation will be outweighed
by the gains during evaluation.
<li><b>When the run-time of an evalution is prohibitive.</b> Compilation itself can reduce evaluation time by up to an
order of magnitude. Futhermore, the compiled component polynomials of an analysis can be distributed easily across
nodes in a cluster or throughout an entire network using middleware such as the Berkeley Open Infrastructure for
Network Computing (BOINC). Evaluations that would take years in a serial context can be performed in days when
widely-distributed.
<li><b>When the same set of markers will be used with varying sets of pedigrees in multiple analyses.</b> Since named
polynomials are specific to pedigree and trait/marker position, analyses that consider various subsets of a group
of pedigrees pay the price of compilation only once for each pedigree and trait/marker position, and then reuse
them without additional cost.
<li><b>When a generated polynomial is extremely large.</b> Polynomials that are larger than physical memory can be
generated successfully, but repeated evaluation becomes very costly as portions outside of physical memory 
must be swapped-in from disk
for every iteration. Running at the boundaries of physical memory also tends to lead to more frequent crashes, so
converting the polynomial to source code as soon as it is generated not only greatly reduces memory consumption
and enhances evaluation performance, but also ensures that the work will not be lost to a crash.
</ul>
<h3>Contraindications</h3>
<ul>
<li>When pedigrees are very simple.
<li>When only a single evaluation will be performed.
</ul>

<h2>Phases in Compiled Polynomial Use</h2>
Compiled polynomial use consists of several phases that have very different requirements and optimization
possibilities:
<ol>

<li><b>Polynomial build and code generation.</b> This is pretty much the same as 
the normal process of polynomial building that occurs whenever
you use the <tt>PE</tt> directive in your analysis, coupled with the fairly quick additional 
step of generating a number of <tt>C</tt>-code files to represent the polynomial. This step is only
performed if there is not already a compiled polynomial of the same name in the current default
directory. As with the normal polynomial
build process, this step can be very memory-intensive depending upon the structure of the pedigrees and number 
of positions in the analysis, and does not benefit significantly from running in
a multi-threaded environment.

<li><b>Code compilation and dynamic library (DL) linking.</b>
Compilation can be performed at the time the source is generated by K<font size="-1">ELVIN</font> itself,
or by way of batch jobs submitted on multiple
nodes. Both approaches require:
<ul>
<li>the GNU C Compiler
<li><tt>polynomial.h</tt> and <tt>polyDLLoop.c</tt> from the distribution <tt>pedlib</tt> directory copied to somewhere on the 
GCC include file search list, i.e. in <tt>/usr/local/include</tt> or in a directory specified
by the <tt>CPATH</tt> environment variable.
<li><tt>compileDL.sh</tt> from the top-level distribution directory copied to somewhere on the command
path, i.e. in a directory specified in the <tt>PATH</tt> environment variable. This script file is
executed from within K<font size="-1">ELVIN</font> to build dynamic libraries, and should be protected from unauthorized
modification, as it would otherwise present a potential security risk. Currently this script moves
source files to a <tt>compiled</tt> subdirectory once a dynamic library is built. You may wish to modify
it's behavior or change the compilation optimization level.
</ul>
This step is only performed if code has been written and synchronous compilation specified. 
<p>
The source files generated are currently sized to be as large as can be compiled in under 16Gb of physical
memory in order to facillitate optimization, and are compiled serially, so this step does not benefit at all from running in a
multi-threaded environment.
<li><b>Compiled polynomial loading and evaluation.</b> An attempt is made to locate and load
a dynamic library (<tt>.so</tt> file) for each polynomial (one per pedigree)
required for evaluation at a specific position before evaluation begins. Only the
current execution directory is searched -- <b>not</b> the normal default dynamic library paths, as moving
the dynamic libraries out of their associated analysis directory would facilitate accidental misuse. Any
polynomials not represented by an existing dynamic library are built, and may be coded, compiled and loaded.
Evaluation of the polynomials for a position is done in parallel by pedigrees, and so takes advantage of
a multi-threaded environment insofar as the pedigrees are of similar complexity.
</ol>
<img border="0" align="right" src="Run Type Performance.jpg" alt="Chart of relative performance">
<h2>K<Font Size="+2">ELVIN</Font> Polynomial Run Types</h2>
As the different phases of compiled polynomial processing have very different performance characteristics,
two distinct mechanisms have been setup in K<font size="-1">ELVIN</font> to support them. The first is a simplistic linear single-run
approach
which does not fully exploit performance capabilities, but can provide a significant performance boost over
regular polynomial processing. The second is a relatively complicated multiple run process that achieves
maximal performance. The chart on the right
illustrates the performance characteristics of these two approaches, along with the characteristics of a non-
polynomial run and a polynomial run without compilation. The illustration assumes a simplified case where there
is a single trait position for which three significant polynomials are 
generated (P1, P2, and P3), two of which are simple 
enough to be described by a single
source file (P1 and P3), and one which requires three source files (P2 F1, F2, and F3).
<p>
The K<font size="-1">ELVIN</font> binaries referenced below can be produced by executing <tt>rebuild.sh</tt> in the top-level
distribution directory. A future release of K<font size="-1">ELVIN</font> will combine them all into a single
binary whose operation will be controlled by command-line options.
<ul>
<li><b>Non-Polynomial Run.</b> This approach uses the <tt>K<font size="-1">ELVIN</font>-normal</tt> binary without the <tt>PE</tt>
directive in the configuration file. It requires very little physical memory, but has a relatively long
execution time which cannot be improved with multi-threading. Total run-time is easily and accurately
extrapolated from initial progress, so a decision can be made fairly quickly as to whether this approach 
is worth pursuing.
<li><b>Normal (No Compilation) Polynomial Run.</b> This approach uses the <tt>K<font size="-1">ELVIN</font>-normal</tt> binary with
the <tt>PE</tt> directive specified in the configuration file. The initial phase - polynomial build - can take
a large amount of memory and only benefits marginally by multi-threading. All pedigree polynomials share
the same in-memory structure, which facillitates simplification. Once the polynomials are
complete, they are evaluated symbolically, which benefits moderately from multi-threading, depending upon the
number of pedigrees and their relative evaluation times.
<li><b>Compiled Single Run.</b> In this approach, a single binary, <tt>K<font size="-1">ELVIN</font>-POLYCOMP_DL</tt> is used to
perform all of the steps of processing, from the initial build thru compilation and evaluation. While this
simplifies the process, it cannot take advantage of distribution of the workload across multiple computers, nor
the association of processing steps with optimal platform characteristics. As each polynomial is built, all of
the files associated with it are compiled serially and then linked into a dynamic library, so the full price
in elapsed time is paid for every compiled polynomial. It is particularly important to pay attention to
trade-offs when considering this type of compiled polynomial run.
<li><b>Compiled Multiple Runs.</b> In this approach, two different <tt>K<font size="-1">ELVIN</font></tt> binaries are used, and the
best results are achieved using multiple machines. The
initial polynomial build (and coding) step is performed all at once by <tt>K<font size="-1">ELVIN</font>-POLYCODE_DL-FAKEEVALUATE</tt>, which,
as the name indicates, does not do a real evaluation of the polynomials as they are only being built in order
to generate source files. This step is best performed on a single-processor machine with a large amount of memory,
as it does not benefit significantly from multiple threads. As soon as the source files for the first polynomial have
been generated, they can start being compiled into object files.
Compilation is performed by <tt>compileDL.sh</tt>, which must be executed either
interactively or in a batch stream for each named polynomial. <tt>compileDL.sh</tt> takes two
parameters which are the name of the polynomial and the optimization level (0 or 1) to be used. If multiple
source files were generated, <tt>compileDL.sh</tt> will compile them sequentially and then link them when finished.
Multiple copies of <tt>compileDL.sh</tt> can be run on a single polynomial, in which case each will compile separate
source files until all are done. In this situation, a final copy must be run to do the link.
<p>
In the illustrated example, there are three
separate machines being used. In practice, our entire cluster of 64 machines has at times been used to quickly
compile and link a large number of polynomials. Once all generated source has been compiled, a
second <tt>K<font size="-1">ELVIN</font></tt> binary, <tt>K<font size="-1">ELVIN</font>-POLYUSE_DL</tt> is run. It uses dynamic libraries whenever 
it finds them instead of building the equivalent polynomials. If no dynamic library is found for a polynomial, the
polynomial will be built-in memory. This step is best performed on
a multi-core machine, and will, in the near future, be able to be distributed by other mechanisms.
</ul>
<h4>BCMM-specific Instructions for Compiled Single Run</h4>
While this approach is the easy way out, it's also the least efficient. If you have very large polynomials, you'll
be waiting a long time for the compiles to complete, and will have to run only on 16Gb nodes.
<p>
Modify whatever shell script you use to run kelvin to:
<ol>
<li>Ensure that a copy of <tt>compileDL.sh</tt> from <tt>pedlib</tt> is somewhere on your path. You might want
to modify the script to e-mail you should a compile fail, instead of emailing Bill.
<li>Ensure that the <tt>CPATH</tt> environment variable includes <tt>/usr/local/src/kelvin-0.37.0/pedlib/:/usr/local/src/kelvin-0.37.0/include/</tt>.
<li>Instead of running regular <tt>kelvin</tt>, run <tt>/usr/local/src/kelvin-0.37.0/kelvin-POLYCOMP_DL</tt>. This
binary is built with the POLYUSE_DL, POLYCODE_DL and POLYCOMP_DL compilation flags set.
</ol>
If a compile blows-up, you'll need to not only fix the problem, which should be described in the <tt>.out</tt> file
for the errant polynomial, but you'll also need to delete the <tt>.compiling</tt> file for that polynomial in order
for a subsequent run to not skip it.
<p>
If you need to build a polynomial that's over 16Gb, use the binary <tt>kelvin-POLYCODE_DL-FAKEEVALUATE-SSD</tt> and
follow <a href="SSDSupport.html">the instructions for using the SSD</a>.
<p>
If you need to restart a run, all you need to do to clean-up is get rid of <tt>.compiling</tt> files, and when you're
using this approach, there should be only one of those. Note that once a dynamic library (<tt>.so</tt> file) is built
for a polynomial, it needn't be built again. All <tt>kelvin-POLY</tt>-whatever programs are smart enough to simply
use existing dynamic libraries and not try to recreate them. If you need to recreate them because the analysis
changed in some fundamental way (see earlier documentation), then you'll need to delete the old <tt>.so</tt> files.
<p>
The default optimization level for compiles is "0" (zero), which compiles fairly fast, but produces dynamic
libraries that run about half as fast as level "1" (one). If you want level "1" (one), you can edit the
<tt>compileDL.sh</tt> script on your path to make it the default. Beware that compilation time will increase
by a factor of 5. Note, however, that if you've already gone thru the generation of dynamic libraries with
optimization level "0", you don't have to regenerate the code to compile at optimization level "1". Just delete
(or move) the old <tt>.so</tt> files, copy the generated source (<tt>.c</tt> and <tt>.h</tt> files) up from
the <tt>compiled</tt> subdirectory, and execute your modified <tt>compileDL.sh</tt> on them.
<p>
Finally, note that after you've gone thru the painful process of compiling all of the dynamic libraries needed
for your analysis, you can start running <tt>kelvin-POLYUSE_DL</tt> to redo the analysis and get better performance
not only because you're not doing compilation anymore, but also because <tt>kelvin-POLYUSE_DL</tt> is built with
OpenMP support, unlike <tt>kelvin-POLYCOMP_DL</tt>. Remember to set <tt>OMP_NUM_THREADS</tt> to something like the
number of available cores to get the most you can, and don't be disappointed if you don't see a big improvement --
our multi-threading is by pedigrees, so if there are only a couple, or they're very unbalanced (one huge one and
a bunch of small ones), it might as well be single-threaded.

<h4>BCMM-specific Instructions for Compiled Multiple Runs</h4>
This approach is best for many large pedigrees, and requires a bit of work. If you only have a few simple pedigrees, stick
with the Compiled Single Run method.
<p>
You won't be using kelvin to do the actual compiles this time, so no special setup is needed. All you need to do is
run <tt>/usr/local/src/kelvin-0.37.0/kelvin-POLYCODE-FAKEEVALUATE</tt> and let it start generating the source
code for polynomials. Note that while it will produce output files, they're garbage (the "FAKEEVALUATE" bit)
because the whole point here is to produce source code as quickly as possible.
<p>
Now, as soon as <tt>.h</tt> files start being produced, you can start running <tt>compileDL.sh</tt> on them. I do
this with a combination of a few scripts that find all <tt>.h</tt> files and automagically create batch jobs 
to execute commands (in this case <tt>compileDL.sh</tt>) on them.
Look for <tt>compile_1</tt> and <tt>compile_2</tt> in my <tt>.bashrc</tt>. I do this multiple times when there
are especially large polynomials involved, because each time <tt>compileDL.sh</tt> runs, it starts working on any
as-yet-uncompiled components of the polynomial, so if there are 20 generated source files for a single polynomial, 
I could have as many as 20 copies of <tt>compileDL.sh</tt> productively running on it. Note that if you do have
multiple copies of <tt>compileDL.sh</tt> running on a single polynomial, they'll compile all of the separate 
source components, but won't do the final link into a dynamic library for fear of missing pieces. Only a
copy of <tt>compileDL.sh</tt> that doesn't skip any uncompiled pieces will attempt a link. This could be corrected
by modifying <tt>compileDL.sh</tt> to do another check for any <tt>.compiling</tt> files once it's done, but
I haven't had a chance to do that yet.
<p>
Once all of the polynomials generated have been compiled -- and you can tell when that's the case by looking to
see if there are any <tt>.h</tt> files left, you can start evaluation. Use <tt>/usr/local/src/kelvin-0.37.0/kelvin-POLYUSE_DL</tt>
with some largish <tt>OMP_NUM_THREADS</tt> for best performance.

<h2>How Compilation Works</h2>
<ul>
<li>Polynomial Structure & Optimizations
<ul>
<li>Term collection and reduction
<li>Subpoly overlap
</ul>
<li>The Polynomial List
<li>Generating Code
<ul>
<li>Loss of overlap
</ul>
<li>Limits to Source Size
<li>Limits to DL Size
<li>Compiler Optimization Performance
</ul>
