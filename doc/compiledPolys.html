<h1>Polynomial Compilation</h1>
<h2>How Does kelvin use Polynomials?</h2>
In order to understand the value of polynomial compilation, one must first understand how kelvin uses
polynomials.
<p>
When kelvin performs an analysis, it is calculating the likelihood of every pedigree's structure
and characteristics for each combination of values in the trait model space, which is normally a range of 
6 gene frequencies, 275 penetrances, and 20 alpha values, or 33,000 combinations of values. This is repeated for
each trait position in the analysis.
<p>
In (original) non-polynomial mode, the pedigree likelihood is explicitly calculated for each combination of 
trait model values via a series of tests, loops and calculations in the compiled code that comprises kelvin.
Since these tests, loops and calculations are all hard-coded in the program, they cannot be 
optimized or simplified in response to the nature of each pedigree, so the same full path is followed
for every combination of values. If the first set of values takes a minute to process, we know that the entire
analysis is going to take tens of thousands or millions of minutes.
<p>
In polynomial mode, we go thru this same hard-coded path in kelvin <i>one time only</i> building a 
symbolic representation of the
calculation using place-holder variables from the trait model space instead of explicit values. In effect, we
build a polynomial that represents the structure and characteristics of the pedigree in terms of the variables from
the trait model space. This polynomial is then extensively simplified and optimized 
using mathematical and computational techniques so that when we substitute-in trait model space values and 
evaluate it, it can perform the calculation 
orders of magnitude faster than the original hard-coded path in kelvin.
<p>
This process of generating polynomials has its own drawbacks. They can take a lot of time and memory to generate
and optimize, and even when
optimization is complete, their evaluation is symbolic and therefore slower than the equivalent binary 
operations. Ideally, we'd like to have both the optimized simplicity of the pedigree/position-specific 
polynomial <i>and</i> the performance and
reusability of compiled code. The answer is to generate and compile code representing each polynomial.
<h2>What Polynomials are Built?</h2>
During a polynomial-mode  analysis, a separate set of likelihood polynomials is built for each pedigree and combination of
relative trait/marker positions. For example, a multipoint analysis with
two pedigrees and three markers where evaluated trait positions occur between every pair of markers as well as before
and after the markers would generate twelve polynomials:
<ul>
<li>trait likelihood for pedigree 1
<li>trait likelihood for pedigree 2
<li>marker set likelihood for M1, M2, M3 and pedigree 1
<li>marker set likelihood for M1, M2, M3 and pedigree 2
<li>combined likelihood for pedigree 1 and trait/marker pattern T-M1-M2-M3
<li>combined likelihood for pedigree 2 and trait/marker pattern T-M1-M2-M3
<li>combined likelihood for pedigree 1 and trait/marker pattern M1-T-M2-M3
<li>combined likelihood for pedigree 2 and trait/marker pattern M1-T-M2-M3
<li>combined likelihood for pedigree 1 and trait/marker pattern M1-M2-T-M3
<li>combined likelihood for pedigree 2 and trait/marker pattern M1-M2-T-M3
<li>combined likelihood for pedigree 1 and trait/marker pattern M1-M2-M3-T
<li>combined likelihood for pedigree 2 and trait/marker pattern M1-M2-M3-T
</ul>
The trait and marker likelihood polynomials are usually extremely simple compared to the combined
likelihood polynomials. Marker likelihood polynomials are usually nothing more than a single variable, while
combined likelihood polynomials usually have tens of thousands, and sometimes even billions of terms.
<p>
<h2>What is Polynomial Compilation?</h2>
Polynomial compilation is a method of post-processing the in-memory polynomials generated by kelvin
so as to facilitate rapid evaluation and reuse.
<p>
Polynomials are built in-memory as usual, and then translated into C-language code and written to one or
more source files which are then compiled into dynamic libraries. These dynamic libraries are
reusable, distributable, compiled representations of the original polynomials that can be
loaded on-demand and evaluated up to ten times faster than their in-memory counterparts. Once
a dynamic library is built for a given polynomial, the relatively slow and memory-intensive process of generating
that polynomial need not be performed again. 

<h2>When to Use Polynomial Compilation</h2>
Generally, you should use polynomial compilation only when it is feasible in your computational environment and
the cost and complexity of compiling and linking dynamic libraries is exceeded by the benefits of rapid
and repeatable evaluation.
<h3>Indications</h3>
<ul>
<li><b>When the ratio of evaluations to trait/marker position changes is high.</b> This is an indication to use
polynomial evaluation in general, and compilation in particular, as the costs of generation will be outweighed
by the gains during evaluation.
<li><b>When the run-time of an evalution is prohibitive.</b> Compilation itself can reduce evaluation time by up to an
order of magnitude. Futhermore, the compiled component polynomials of an analysis can be distributed easily across
nodes in a cluster or throughout an entire network using middleware such as the Berkeley Open Infrastructure for
Network Computing (BOINC). Evaluations that would take years in a serial context can be performed in days when
widely-distributed.
<li><b>When the same set of markers will be used with varying sets of pedigrees in multiple analyses.</b> Since named
polynomials are specific to pedigree and trait/marker position, analyses that consider various subsets of a group
of pedigrees pay the price of compilation only once for each pedigree and trait/marker position, and then reuse
them without additional cost.
<li><b>When a generated polynomial is extremely large.</b> Polynomials that are larger than physical memory can be
generated successfully, but repeated evaluation becomes very costly as portions outside of physical memory 
must be swapped-in from disk
for every iteration. Running at the boundaries of physical memory also tends to lead to more frequent crashes, so
converting the polynomial to source code as soon as it is generated not only greatly reduces memory consumption
and enhances evaluation performance, but also ensures that the work will not be lost to a crash.
</ul>
<h3>Contraindications</h3>
<ul>
<li>When pedigrees are very simple.
<li>When only a single evaluation will be performed.
</ul>

<h2>Phases in Polynomial Compilation (IN PROGRESS)</h2>
Polynomial compilation consists of three phases that have very different requirements and optimization
possibilities:
<ol>
<li><b>Polynomial build and code generation.</b> This is the normal process of polynomial building that occurs whenever
you use the <tt>PE</tt> directive in your analysis, coupled with the fairly quick additional 
step of generating a number of <tt>C</tt>-code files to represent the polynomial. As with the normal polynomial
build process, this step can be very memory-intensive depending upon the structure of the pedigrees and number 
of positions in the analysis, and does not benefit significantly from running in
a multi-threaded environment.
<li><b>Code compilation and dynamic library (DL) linking.</b>
Compilation can be performed at the time the source is generated by kelvin itself,
or by way of batch jobs submitted on multiple
nodes. These two approaches are incompatible because if kelvin is doing the compiling, it expects that when
it gets done there will be a ready-to-load dynamic library. This will not be the case if the compilation is
being distributed to multiple nodes via batch jobs. There are, therefore, two ways to go:
<ol>
<li> Run kelvin-POLYCOMP_DL, which will generate, compile, link and use dynamic libraries all by itself, or
<li> Use a multi-step approach:
<ol>
<li>Run kelvin-POLYCODE_DL-FAKEEVALUATE, which will generate the code for dynamic libraries, but neither
compile nor try to load and use them. This is platform-independent, possibly memory-intensive, and single-
threaded.
<li>Compile (and ultimately link) all of the source files generated. This can be done on any 
linux nodes that are hardware-compatible
with the platform to be used in the evaluation step. It can be performed thoroughly in parallel, and even
started while the previous step is still running so long as polynomial code is available to compile.
<li>Run kelvin-POLYUSE_DL. Once all generated polynomials are compiled and linked, kelvin-POLYUSE_DL will be
able to load and use them instead of generating the polynomials. This step uses only a fraction
(generally a third or less) of the memory required to hold a normal polynomial, and allows rapid, multi-
threaded evaluation of the separate pedigree polynomials for each position.
</ol>
</ol>
</ol>
<img border="0" align="right" src="Run Type Performance.jpg" alt="Chart of relative performance">
<h2>Kelvin Polynomial Run Types</h2>
As the different phases of compiled polynomial processing have very different performance characteristics,
two distinct mechanisms have been setup in kelvin to support them. The first is a simplistic linear single-run
approach
which does not fully exploit performance capabilities, but can provide a significant performance boost over
regular polynomial processing. The second is a relatively complicated multiple run process that achieves
maximal performance. The chart on the right
illustrates the performance characteristics of these two approaches, along with the characteristics of a non-
polynomial run and a polynomial run without compilation. The illustration assumes a simplified case where there
is a single trait position for which three significant polynomials are 
generated (P1, P2, and P3), two of which are simple 
enough to be described by a single
source file (P1 and P3), and one which requires three source files (P2 F1, F2, and F3).
<ul>
<li><b>Non-Polynomial Run.</b> This approach uses the <tt>kelvin-normal</tt> binary without the <tt>PE</tt>
directive in the configuration file. It requires very little physical memory, but has a relatively long
execution time which cannot be improved with multi-threading. Total run-time is easily and accurately
extrapolated from initial progress, so a decision can be made fairly quickly as to whether this approach 
is worth pursuing.
<li><b>Normal (No Compilation) Polynomial Run.</b> This approach uses the <tt>kelvin-normal</tt> binary with
the <tt>PE</tt> directive specified in the configuration file. The initial phase - polynomial build - can take
a large amount of memory and only benefits marginally by multi-threading. All pedigree polynomials share
the same in-memory structure, which facillitates simplification. Once the polynomials are
complete, they are evaluated symbolically, which benefits moderately from multi-threading, depending upon the
number of pedigrees and their relative evaluation times.
<li><b>Compiled Single Run.</b> In this approach, a single binary, <tt>kelvin-POLYCOMP_DL</tt> is used to
perform all of the steps of processing, from the initial build thru compilation and evaluation. While this
simplifies the process, it cannot take advantage of distribution of the workload across multiple computers, nor
the association of processing steps with optimal platform characteristics. As each polynomial is built, all of
the files associated with it are compiled serially and then linked into a dynamic library, so the full price
in elapsed time is paid for every compiled polynomial. It is particularly important to pay attention to
trade-offs when considering this type of compiled polynomial run.
<li><b>Compiled Multiple Runs.</b> In this approach, two different <tt>kelvin</tt> binaries are used, and the
best results are achieved using multiple machines. The
initial polynomial build (and coding) step is performed all at once by <tt>kelvin-POLYCODE_DL-FAKEEVALUATE</tt>, which,
as the name indicates, does not do a real evaluation of the polynomials as they are only being built in order
to generate source files. This step is best performed on a single-processor machine with a large amount of memory,
as it does not benefit significantly from multiple threads. As soon as the source files for the first polynomial have
been generated, they can start being compiled into object files on other machines. If a polynomial produces multiple
source files, they can all be compiled separately and simultaneously. In the illustrated example, there are three
separate machines being used. In practice, our entire cluster of 64 machines has at times been used to quickly
compile and link a large number of polynomial dynamic libraries. Once all generated source has been compiled, the
second <tt>kelvin</tt> binary, <tt>kelvin-POLYUSE_DL</tt> is run. It uses dynamic libraries instead whenever 
it finds them instead of building the equivalent polynomials. If no dynamic library is found for a polynomial, the
polynomial will be built-in memory and evaluated as by <tt>kelvin-normal</tt>. This step is best performed on
a multi-core machine, and will, in the near future, be able to be distributed by other mechanisms.
</ul>
<p>

<h2>How Compilation Works</h2>
<ul>
<li>Polynomial Structure & Optimizations (from Hongling's paper)
<ul>
<li>Term collection and reduction
<li>Subpoly overlap
</ul>
<li>The Polynomial List
<li>Generating Code
<ul>
<li>Loss of overlap
</ul>
<li>Limits to Source Size
<li>Limits to DL Size
<li>Compiler Optimization Performance
</ul>
